[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2024-04-03_duckdb-r-sentinel-data/index.html",
    "href": "posts/2024-04-03_duckdb-r-sentinel-data/index.html",
    "title": "Manipulating Sentinel Data with DuckDB",
    "section": "",
    "text": "At work, we primarily work in SAS. This due to SAS as being a validated language and because of the large size of the data we work with. That said, this is 2024—SAS licenses are extremely expensive and the language is not nearly as flexible as open source offerings.\nI generally prefer working in R, but one of the biggest challenges I’ve faced in using R at work is the fact that while SAS is able to handle data sets that are larger than memory by using a combination of fast disk I/O and memory management, R—like Python, natively prefers to manipulate data sets in memory. This is challenging when the datasets can be 100s of gigabytes in in question.\nI’ve recently heard a lot of good things about DuckDB and been reading through @hrbrmstr’s Cooking with DuckDB. DuckDB is a relational database system designed for Online analytic processing (OLAP) workloads. It can be compiled with no external dependencies—incredibly useful for embedded applications—and is known for its ability to query large databases very quickly. I decided to give it a try. This post is intended to do some basic benchmarking against data in the Sentinel Common Data Model (SCDM). It has R, Python, and Rust APIs, which will make it incredibly useful for querying large datasets from these languages1."
  },
  {
    "objectID": "posts/2024-04-03_duckdb-r-sentinel-data/index.html#getting-started",
    "href": "posts/2024-04-03_duckdb-r-sentinel-data/index.html#getting-started",
    "title": "Manipulating Sentinel Data with DuckDB",
    "section": "Getting Started",
    "text": "Getting Started\nWhile it’s not nearly as large as the data our Data Partners hold or our internal test data, the CMS 2008-2010 Data Entrepreneurs’ Synthetic Public Use Files (SynPUFs) are publically available synthetic data in the Sentinel Common Data Model (SCDM) format, able to be used publicly, and still of a decent size. The SynPUFs are 5% synthetic samples of Medicare claims data from 2008 to 2010 and consist of 20, mutually exclusive datasets in the SAS7BDAT format.\nThe first challenge is that these datasets are large—e.g., the diagnosis table consists of twenty 810 MB .SAS7BDAT format files. As SAS7BDAT is a propriety format and I don’t have a SAS license on my home machine, I needed to convert these files to a format that DuckDB could read. The haven library offers read_sas(), but it requires holding the data in memory, and with 16 Gb of RAM on my MBP, binding the the subsamples for the largest datasets together and writing to parquet was a non-starter.\nFortunately, thanks to R’s purrr(), I was able to devise an inelegant solution2. I didn’t bother benchmarking this step, but it took roughly an hour on the MBP to chew through all of the datasets, appending each partition to a DuckDB table one-by-one, and then writing to parquet. Listing 1 shows the code used for this.\n\n\n\nListing 1: R code for extracting SAS datasets and converting to parquet\n\ntables &lt;- c(\"death\", \"demographic\", \"diagnosis\", \"dispensing\", \"encounter\", \"enrollment\",\n    \"procedure\", \"facility\", \"provider\")\n\ncon &lt;- DBI::dbConnect(duckdb::duckdb(dbdir = \"~/dev/SynPUFsDuckDB/data/duckdb.duckdb\"))\n\npurrr::walk(tables, function(x) {\n\n    data &lt;- haven::read_sas(paste0(\"data/\", x, \"_1\", \".sas7bdat\"))\n\n    DBI::dbExecute(con, \"drop table if exists data\")\n    duckdb::dbWriteTable(con, \"data\", data)\n\n    num &lt;- 2:20\n\n    purrr::walk(num, function(y) {\n        data &lt;- haven::read_sas(paste0(\"data/\", x, \"_\", y, \".sas7bdat\"))\n        duckdb::dbAppendTable(con, \"data\", data)\n    })\n\n    DBI::dbExecute(con, sprintf(\"copy data to '~/dev/SynPUFsDuckDB/data/%s-snappy.parquet' (format 'parquet');\",\n        x))\n})\n\nDBI::dbExecute(con, \"drop table if exists data\")\nDBI::dbDisconnect(con, shutdown = TRUE)\n\n\n\n\nNow that we have that data in a more usable format. Let’s look at what we have. I’ll start by summarizing the tables. DuckDB’s summarize feature is useful, but overkill here, so we’re going simply going to describe the tables.\n\n\n\n\n\n\nConventions\n\n\n\nWhere it makes sense, I’m calling SAS and DuckDB via the command line, using time. This outputs Real, User, and Sys time below the code block or resulting table. For the purposes of most folks, Real time matters the most. For a brief explanation on what each means, please see this stackoverflow post.\n\n\n\n\nENR\nDEM\nDIS\nENC\nDIA\nPRO\nDTH\nFAC\nPVD\n\n\n\n\ntime /opt/homebrew/bin/duckdb duckdb.duckdb -markdown -c \"\n  DESCRIBE SELECT *\n  FROM '~/dev/SynPUFsDuckDB/data/enrollment-snappy.parquet';\"\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\nPatID\nUBIGINT\nYES\n\n\n\n\n\nEnr_Start\nDATE\nYES\n\n\n\n\n\nEnr_End\nDATE\nYES\n\n\n\n\n\nMedCov\nVARCHAR\nYES\n\n\n\n\n\nDrugCov\nVARCHAR\nYES\n\n\n\n\n\nChart\nVARCHAR\nYES\n\n\n\n\n\n\nreal 0m0.012s user 0m0.008s sys 0m0.002s\n\n\n\n\ntime /opt/homebrew/bin/duckdb duckdb.duckdb -markdown -c \"\n  DESCRIBE SELECT *\n  FROM '~/dev/SynPUFsDuckDB/data/demographic-snappy.parquet';\"\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\nPatID\nUBIGINT\nYES\n\n\n\n\n\nBirth_date\nDATE\nYES\n\n\n\n\n\nSex\nVARCHAR\nYES\n\n\n\n\n\nHispanic\nVARCHAR\nYES\n\n\n\n\n\nRace\nVARCHAR\nYES\n\n\n\n\n\nPostalCode\nVARCHAR\nYES\n\n\n\n\n\nPostalCode_date\nDATE\nYES\n\n\n\n\n\n\nreal 0m0.011s user 0m0.007s sys 0m0.002s\n\n\n\n\ntime /opt/homebrew/bin/duckdb duckdb.duckdb -markdown -c \"\n  DESCRIBE SELECT *\n  FROM '~/dev/SynPUFsDuckDB/data/dispensing-snappy.parquet';\"\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\nPatID\nUBIGINT\nYES\n\n\n\n\n\nProviderID\nUBIGINT\nYES\n\n\n\n\n\nRxDate\nDATE\nYES\n\n\n\n\n\nRx\nVARCHAR\nYES\n\n\n\n\n\nRx_CodeType\nVARCHAR\nYES\n\n\n\n\n\nRxSup\nUINTEGER\nYES\n\n\n\n\n\nRxAmt\nUINTEGER\nYES\n\n\n\n\n\n\nreal 0m0.016s user 0m0.012s sys 0m0.002s\n\n\n\n\ntime /opt/homebrew/bin/duckdb duckdb.duckdb -markdown -c \"\n  DESCRIBE SELECT *\n  FROM '~/dev/SynPUFsDuckDB/data/encounter-snappy.parquet';\"\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\nPatID\nUBIGINT\nYES\n\n\n\n\n\nEncounterID\nUBIGINT\nYES\n\n\n\n\n\nADate\nDATE\nYES\n\n\n\n\n\nDDate\nDATE\nYES\n\n\n\n\n\nEncType\nVARCHAR\nYES\n\n\n\n\n\nFacilityID\nUBIGINT\nYES\n\n\n\n\n\nDischarge_Disposition\nVARCHAR\nYES\n\n\n\n\n\nDischarge_Status\nVARCHAR\nYES\n\n\n\n\n\nDRG\nVARCHAR\nYES\n\n\n\n\n\nDRG_Type\nVARCHAR\nYES\n\n\n\n\n\nAdmitting_Source\nVARCHAR\nYES\n\n\n\n\n\n\nreal 0m0.019s user 0m0.016s sys 0m0.002s\n\n\n\n\ntime /opt/homebrew/bin/duckdb duckdb.duckdb -markdown -c \"\n  DESCRIBE SELECT *\n  FROM '~/dev/SynPUFsDuckDB/data/diagnosis-snappy.parquet';\"\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\nPatID\nUBIGINT\nYES\n\n\n\n\n\nEncounterID\nUBIGINT\nYES\n\n\n\n\n\nADate\nDATE\nYES\n\n\n\n\n\nProviderID\nUBIGINT\nYES\n\n\n\n\n\nEncType\nVARCHAR\nYES\n\n\n\n\n\nDX\nVARCHAR\nYES\n\n\n\n\n\nDX_codetype\nVARCHAR\nYES\n\n\n\n\n\nOrigDX\nVARCHAR\nYES\n\n\n\n\n\nPDX\nVARCHAR\nYES\n\n\n\n\n\nPAdmit\nVARCHAR\nYES\n\n\n\n\n\n\nreal 0m0.040s user 0m0.034s sys 0m0.004s\n\n\n\n\ntime /opt/homebrew/bin/duckdb duckdb.duckdb -markdown -c \"\n  DESCRIBE SELECT *\n  FROM '~/dev/SynPUFsDuckDB/data/procedure-snappy.parquet';\"\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\nPatID\nUBIGINT\nYES\n\n\n\n\n\nEncounterID\nUBIGINT\nYES\n\n\n\n\n\nADate\nDATE\nYES\n\n\n\n\n\nProviderID\nUBIGINT\nYES\n\n\n\n\n\nEncType\nVARCHAR\nYES\n\n\n\n\n\nPX\nVARCHAR\nYES\n\n\n\n\n\nPX_codetype\nVARCHAR\nYES\n\n\n\n\n\nOrigPX\nVARCHAR\nYES\n\n\n\n\n\n\nreal 0m0.026s user 0m0.021s sys 0m0.003s\n\n\n\n\ntime /opt/homebrew/bin/duckdb duckdb.duckdb -markdown -c \"\n  DESCRIBE SELECT *\n  FROM '~/dev/SynPUFsDuckDB/data/death-snappy.parquet';\"\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\nPatID\nUBIGINT\nYES\n\n\n\n\n\nDeathDt\nDATE\nYES\n\n\n\n\n\nDtImpute\nVARCHAR\nYES\n\n\n\n\n\nSource\nVARCHAR\nYES\n\n\n\n\n\nConfidence\nVARCHAR\nYES\n\n\n\n\n\n\nreal 0m0.011s user 0m0.007s sys 0m0.002s\n\n\n\n\ntime /opt/homebrew/bin/duckdb duckdb.duckdb -markdown -c \"\n  DESCRIBE SELECT *\n  FROM '~/dev/SynPUFsDuckDB/data/facility-snappy.parquet';\"\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\nFacilityID\nUBIGINT\nYES\n\n\n\n\n\nFacility_Location\nVARCHAR\nYES\n\n\n\n\n\n\nreal 0m0.010s user 0m0.007s sys 0m0.002s\n\n\n\n\ntime /opt/homebrew/bin/duckdb duckdb.duckdb -markdown -c \"\n  DESCRIBE SELECT *\n  FROM '~/dev/SynPUFsDuckDB/data/provider-snappy.parquet';\"\n\n\ncolumn_name\ncolumn_type\nnull\nkey\ndefault\nextra\n\n\n\nProviderID\nUBIGINT\nYES\n\n\n\n\n\nSpecialty\nVARCHAR\nYES\n\n\n\n\n\nSpecialty_CodeType\nVARCHAR\nYES\n\n\n\n\n\n\nreal 0m0.011s user 0m0.007s sys 0m0.002s\n\n\n\n\nWhen I initially ran this, I discovered a new challenge: As R doesn’t natively have a 8-byte (64-bit) numeric type, it coerced any column that was an integer larger than 8-bytes to a double. I didn’t want to take the chance of out of memory errors to work with these as a dataframe, so I used DuckDB to alter the type. UBIGINT is DuckDB’s unsigned 8-byte integer type.\nLet’s count grab the number of records in each parquet file and unioning them together:\n\ntime /opt/homebrew/bin/duckdb duckdb.duckdb -markdown -c \"\n  SELECT format('Enrollment') as Table\n       , count(*) as count\n  FROM '~/dev/SynPUFsDuckDB/data/enrollment-snappy.parquet'\n  UNION\n  SELECT format('Demographic') as Table\n       , count(*) as count\n  FROM '~/dev/SynPUFsDuckDB/data/demographic-snappy.parquet'\n  UNION\n  SELECT format('Dispensing') as Table\n       , count(*) as count\n  FROM '~/dev/SynPUFsDuckDB/data/dispensing-snappy.parquet'\n  UNION\n  SELECT format('Encounter') as Table\n       , count(*) as count\n  FROM '~/dev/SynPUFsDuckDB/data/encounter-snappy.parquet'\n  UNION\n  SELECT format('Diagnosis') as Table\n       , count(*) as count\n  FROM '~/dev/SynPUFsDuckDB/data/diagnosis-snappy.parquet'\n  UNION\n  SELECT format('Procedure') as Table\n       , count(*) as count\n  FROM '~/dev/SynPUFsDuckDB/data/procedure-snappy.parquet'\n  UNION\n  SELECT format('Death') as Table\n       , count(*) as count\n  FROM '~/dev/SynPUFsDuckDB/data/death-snappy.parquet'\n  UNION\n  SELECT format('Facility') as Table\n       , count(*) as count\n  FROM '~/dev/SynPUFsDuckDB/data/facility-snappy.parquet'\n  UNION\n  SELECT format('Provider') as Table\n       , count(*) as count\n  FROM '~/dev/SynPUFsDuckDB/data/provider-snappy.parquet'\n  ;\"\n\n\nTable\ncount\n\n\n\nDeath\n98152\n\n\nDemographic\n2224739\n\n\nProvider\n12803343\n\n\nFacility\n53686\n\n\nEncounter\n111738882\n\n\nDispensing\n111085327\n\n\nEnrollment\n3668891\n\n\nDiagnosis\n352441529\n\n\nProcedure\n222919598\n\n\n\nreal 0m0.129s user 0m0.185s sys 0m0.025s\n\nAs this tells you, we’re working with tables that have between ~53K to 350+ M records. Hardly large compared to some of the real datasets we might work with, but reasonably sized for benchmarking."
  },
  {
    "objectID": "posts/2024-04-03_duckdb-r-sentinel-data/index.html#querying-the-data",
    "href": "posts/2024-04-03_duckdb-r-sentinel-data/index.html#querying-the-data",
    "title": "Manipulating Sentinel Data with DuckDB",
    "section": "Querying the Data",
    "text": "Querying the Data\nAs I explored DuckDB, I ran a number of different SQL queries—some of which had practical purposes, others which were essentially cursed—as I attempted to put the tool through its passes. I was consistently impressed by the speed at which it returned answers. I decided to start with a relatively simple query of some practical use. Listing 2 illustrates SAS code which does the following:\n\nCreate a SAS format which bins ages into age cohorts\nCalculates age as of the maximum date of the data—2010-12-31, stratifies by Sex, Ethnicity, and Race, and counts the number of records of a given age.\nApplies the label and re-aggregates count.\nRemoves Sex, Ethnicity, and Race from the dataset, and re-aggregates the count.\n\nFor reference, the demographic table is smaller than the diagnosis table with:\n\n\nListing 2: Calulate age cohorts and aggregate the demographic table by age cohort and demographic characteristics.\n\nlibname synpufs \"/path/obscured/SynPUFsDuckDB/data/\";\n\nproc format;\n  value agecat_years\n  .       = \"00. Missing\"\n  low-&lt;0  = \"00. Negative\"\n  0-&lt;2    = \"01. 0-1 yrs\"\n  2-&lt;5    = \"02. 2-4 yrs\"\n  5-&lt;10   = \"03. 5-9 yrs\"\n  10-&lt;15  = \"04. 10-14 yrs\"\n  15-&lt;19  = \"05. 15-18 yrs\"\n  19-&lt;22  = \"06. 19-21 yrs\"\n  22-&lt;25  = \"07. 22-24 yrs\"\n  25-&lt;35 = \"08. 25-34 yrs\"\n  35-&lt;45 = \"09. 35-44 yrs\"\n  45-&lt;55 = \"10. 45-54 yrs\"\n  55-&lt;60 = \"11. 55-59 yrs\"\n  60-&lt;65 = \"12. 60-64 yrs\"\n  65-&lt;70 = \"13. 65-69 yrs\"\n  70-&lt;75 = \"14. 70-74 yrs\"\n  75-high = \"15. 75+ yrs\"\n  ;\nrun;\n\nproc sql noprint;\ncreate table _dem_l2_age as\nselect floor((intck(\"month\",birth_date,\"31Dec2010\"d)-(day(\"31Dec2010\"d)&lt;day(birth_date)))/12) as age_years label=\"Age (Years)\"\n     ,  sex\n     ,  hispanic\n     ,  race\n     ,  count(*) as count format=comma16.\nfrom synpufs.demographic\ngroup by calculated age_years, sex, hispanic, race;\nquit;\n\nproc sql noprint;\n     create table dem_l2_agecat_catvars as\n     select put(age_years,agecat_years.) as agecat_years label=\"Age Category (Years)\"\n     ,  sex\n     ,  hispanic\n     ,  race\n     ,  sum(count) as count format=comma16.\n     from _dem_l2_age\n     group by calculated agecat_years, sex, hispanic, race\n     order by calculated agecat_years;\nquit;\n\nproc sql noprint;\n     create table dem_l2_agecat as\n     select agecat_years\n     ,  sum(count) as count format=comma16.\n     from dem_l2_agecat_catvars\n     group by agecat_years;\nquit;\n\n\n\nI called this on the SAS Server via the CLI with time, and got the following results:\nreal    0m5.178s\nuser    0m1.871s\nsys     0m0.153s\nNow let’s run the same query via DuckDB in the command line—note the time below Listing 3.\n\n\n\nListing 3: Calulate age cohorts and aggregate the demographic table by age cohort and demographic characteristics.\n\ntime /opt/homebrew/bin/duckdb duckdb.duckdb -markdown -c \"\nCREATE OR REPLACE TEMP TABLE dem_l3_tmp AS\nSELECT (CAST(FLOOR(CAST(DATESUB('month', Birth_date, '2010-12-31') AS INTEGER) / 12) AS INTEGER)) as age\n      , sex\n      , hispanic\n      , race\n      , count(*) as count\nFROM '~/dev/SynPUFsDuckDB/data/demographic-snappy.parquet'\nGROUP BY age, sex, hispanic, race;\n\nCREATE OR REPLACE TEMP TABLE dem_l3_agecat_catvars AS\nSELECT CASE WHEN age IS NULL THEN '00. MISSING'\n            WHEN age &lt; 0 THEN '00. NEGATIVE'\n            WHEN AGE BETWEEN 2 AND 4   THEN '02. 2-4 yrs'\n            WHEN AGE BETWEEN 5 AND 9  THEN '03. 5-9 yrs'\n            WHEN AGE BETWEEN 10 AND 14 THEN '04. 10-14 yrs'\n            WHEN AGE BETWEEN 15 AND 18 THEN '05. 15-18 yrs'\n            WHEN AGE BETWEEN 19 AND 21 THEN '06. 19-21 yrs'\n            WHEN AGE BETWEEN 22 AND 24 THEN '07. 22-24 yrs'\n            WHEN AGE BETWEEN 25 AND 34 THEN '08. 25-34 yrs'\n            WHEN AGE BETWEEN 35 AND 44 THEN '09. 35-44 yrs'\n            WHEN AGE BETWEEN 45 AND 54 THEN '10. 45-54 yrs'\n            WHEN AGE BETWEEN 55 AND 59 THEN '11. 55-59 yrs'\n            WHEN AGE BETWEEN 60 AND 64 THEN '12. 60-64 yrs'\n            WHEN AGE BETWEEN 65 AND 69 THEN '13. 65-69 yrs'\n            WHEN AGE BETWEEN 70 AND 74 THEN '14. 70-74 yrs'\n            ELSE '15. 75+' END as agecat_years\n     , sex\n     , hispanic\n     , race\n     , sum(count) as count\nFROM dem_l3_tmp\nGROUP BY agecat_years, sex, hispanic, race\nORDER BY agecat_years;\n\nCREATE OR REPLACE TABLE dem_l3_agecat AS\n  SELECT\n    agecat_years,\n    SUM(count) AS count\n  FROM\n    dem_l3_agecat_catvars\n  GROUP BY\n    agecat_years\n  ORDER BY\n    agecat_years;\n\"\n\n\n\n\n\nreal    0m0.059s\nuser    0m0.283s\nsys 0m0.012s\n\n\nAs you can see, this results in a runtime that is a fraction of what SAS is delivering. For the sake of completeness, let’s compare the results.\n\n\nDuckDB\nSAS\n\n\n\n\n\n\nagecat_years\ncount\n\n\n\n08. 25-34 yrs\n24747\n\n\n09. 35-44 yrs\n49660\n\n\n10. 45-54 yrs\n100906\n\n\n11. 55-59 yrs\n70964\n\n\n12. 60-64 yrs\n80540\n\n\n13. 65-69 yrs\n342196\n\n\n14. 70-74 yrs\n460971\n\n\n15. 75+\n1094755\n\n\n\n\n\n\nObs    agecat_years                count\n\n1     08. 25-34 yrs              24,747\n2     09. 35-44 yrs              49,660\n3     10. 45-54 yrs             100,906\n4     11. 55-59 yrs              70,964\n5     12. 60-64 yrs              80,540\n6     13. 65-69 yrs             342,196\n7     14. 70-74 yrs             460,971\n8     15. 75+ yrs             1,094,755\n\n\n\nLet’s try something a little more cursed. Listing 4 shows a query which attempts to calculate the number of diagnoses per Encounter per Patient. It’s going to output 111706063 rows, so I’m not going to bother printing results, just the runtime.\n\n\n\nListing 4: Runtime for a query that tries to join the diagnosis table to the encounter table by PatID and EncounterID and counts the number of diagnosis by PatID and EncounterID\n\ntime /opt/homebrew/bin/duckdb duckdb.duckdb -c \"\n  CREATE OR REPLACE TEMP TABLE dia_l2_enc_patid AS\n  SELECT a.PatID\n       , a.EncounterID\n       , count(b.DX) as count \n  FROM '~/dev/SynPUFsDuckDB/data/encounter-snappy.parquet' as a \n  JOIN '~/dev/SynPUFsDuckDB/data/diagnosis-snappy.parquet' as b \n  ON (a.PatID = b.PatID and a.EncounterID = b.EncounterID) \n  GROUP BY a.EncounterID, a.PatID;\"\n\n\n\n\n\nreal    0m12.780s\nuser    0m50.538s\nsys 0m12.321s\n\n\nWe’re joining a table with 352,441,529 records to a table with 111,738,882 M records. Each time I run this query on my laptop, it takes seconds. I gave up on SAS after about 45 minutes."
  },
  {
    "objectID": "posts/2024-04-03_duckdb-r-sentinel-data/index.html#final-thoughts",
    "href": "posts/2024-04-03_duckdb-r-sentinel-data/index.html#final-thoughts",
    "title": "Manipulating Sentinel Data with DuckDB",
    "section": "Final Thoughts",
    "text": "Final Thoughts\nDuckDB yields amazing performance even on machine without a ton of horsepower and it’s ability to quickly query larger than memory datasets will make it much easier to work with larger than memory datasets in languages like R and Python. The fact that it is designed for embedded applications with minimal external dependencies makes it a good candidate for deployment in distributed research networks, and it’s use of fairly standard SQL makes migrating to it incredibly easy.\nDuckDB is magic.\nWhat about R?\nThere are a number of methods for running a query and returning an R object. They deserve their own post."
  },
  {
    "objectID": "posts/2024-04-03_duckdb-r-sentinel-data/index.html#footnotes",
    "href": "posts/2024-04-03_duckdb-r-sentinel-data/index.html#footnotes",
    "title": "Manipulating Sentinel Data with DuckDB",
    "section": "Footnotes",
    "text": "Footnotes\n\nI am aware of arrow and pola.rs and I will eventually benchmark these versus DuckDB.↩︎\nA better solution would have been to use SAS to convert the SynPUF data from the SAS7BDAT format to JSON or CSV, however, I don’t have a SAS license for my personal machine, my work laptop doesn’t have the hard drive space left to hold it, and it VPN and bandwidth constraints would have made it challenging to remove from my work environment.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "musings-mountainherder-xyz",
    "section": "",
    "text": "Manipulating Sentinel Data with DuckDB\n\n\n\n\n\n\nduckdb\n\n\nSAS\n\n\nSentinel\n\n\n\nDocumenting and benchmarking DuckDB for manipulating large datasets in the Sentinel Common Data Model. \n\n\n\n\n\nMar 31, 2024\n\n\nD. Scarnecchia\n\n\n\n\n\n\nNo matching items"
  }
]